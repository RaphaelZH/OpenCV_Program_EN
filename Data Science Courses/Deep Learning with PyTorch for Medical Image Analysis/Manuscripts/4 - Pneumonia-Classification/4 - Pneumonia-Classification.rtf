{\rtf1\ansi\ansicpg1252\cocoartf2759
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fnil\fcharset0 HelveticaNeue;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww17080\viewh9100\viewkind0
\deftab560
\pard\pardeftab560\slleading20\partightenfactor0

\f0\fs26 \cf0 # `nn.BCELoss` creates a criterion that measures the Binary Cross Entropy between the target and the input probabilities\
\
# The log-sum-exp trick helps prevent underflow/overflow errors, and is in essence just taking advantage of mathematical properties to reduce underflow/overflow by using the log-sum-exp function which computes a smoothed maximum, i.e., a smoothed approximation of the maximum value function, and is mainly used in machine learning algorithms\
\
\
\
\
\
# The parameter `pos_weight` denotes the weight of the positive example to be broadcast with the target, which must be a tensor transformed by a list whose length can be 1 or equal to batch size\
\
# In short, the parameter `pos_weight` is a scalar indicating the weight of the positive class\
\
# As can be seen from the visual comparisons above, there are more images without signs of pneumonia than there are images with signs of pneumonia, which means that the classes in these datasets are imbalanced\
\
# There are several ways to deal with imbalanced datasets, and weighted loss is one of them; the parameter `pos_weight` provides a way to customize the weighted loss\
\
# The `Accuracy` class in the `torchmetrics` module calculates accuracy by setting the parameter `task` to either "binary", "multiclass" or "multilabel"\
\
# Convert labels to float type for use in calculating loss\
\
# Here, the instance of the class `PneumoniaModel` inherited from the PyTorch Lightning module is invoked as a function, which is actually a shorthand form of invoking the `self.__call__` function\
\
# Need to make sure that the predictions and labels have the same shape, in this case `self(x_ray)[:, 0]` works the same as `self(x_ray).view(-1)`\
\
# Logs can be logged from anywhere in the LightningModule and its callbacks by using the `log` or `log_dict` method\
\
# Batch loss and batch accuracy of the processed training dataset are logged here\
\
# The parameters`on_step`, `on_epoch`, and `prog_bar` in the `log` or `log_dict` method respectively correspond to whether the log is logged at this step, whether the accumulated metrics are logged epoch by epoch, and whether the log is logged to the progress bar\
\
Batch Training Loss\
\
\
\
\
\
\
\
\
\
\
# Since the prediction type is binary, this accuracy function expects the label type to be an integer\
\
# Note that the default setting of the parameter `threshold` in the `Accuracy` class of the `torchmetrics` module is 0.5, indicating the threshold for transforming probabilities into binary predictions (i.e., 0, 1), but the current threshold for the predicted probabilities obtained from the training model is 0, so to resolve this issue, use the `torch.sigmoid` function to convert the current probability threshold from 0 to 0.5 to make the interval of probability between 0 and 1\
\
Batch Training Accuracy\
\
# At the end of the `training_step` function, as with the `validation_step` function, it is possible to return a tensor of loss or predictions, or a dictionary containing the loss and predictions, etc., if it is needed for later in training\
\
# Importantly, when the internal variable `automatic_optimization` is set to the default value of True, the loss returned from the `training_step` function will be called to perform backward operations via the internal `backward` function\
\
# This is why the return of loss from the `training_step` function cannot be skipped, even if the return is a dictionary in which the key "loss" and the value loss must be preserved in this dictionary\
\
# After one epoch the overall loss and accuracy of the processed training dataset is computed\
\
Epoch Training Loss\
\
# Use the `clear` method to remove all elements from the list to free up memory\
\
Epoch Training Accuracy\
\
# The `compute` function is used to compute a final value from the state of the metric, and can be used when implementing custom metrics\
\
# In general, the settings used in the `validation_step` function are almost identical to those used in the `training_step` function\
\
# Batch loss and batch accuracy of the processed validation dataset are logged here\
\
Batch Validation Loss\
\
Batch Validation Accuracy\
\
# After one epoch the overall loss and accuracy of the processed validation dataset is computed\
\
Epoch Validation Loss\
\
Epoch Validation Accuracy\
\
# The `configure_optimizers` function selects which optimizers and learning rate schedulers to use in model optimization\
\
# Normally only one optimizer is needed, in some cases more than one may be needed, but optimization with multiple optimizers is only available in manual optimization mode\
\
# The return value of the `configure_optimizers` function can be an optimizer, a list of optimizers or a tuple of optimizers, two lists, a dictionary, or None (in which case no optimizer is used at runtime)\
\
# If the parameter `class_weight` is set to "balanced", `compute_class_weight` calculates the class weight according to the following formula: `n_samples / (n_classes * np.bincount(y))` where `np.bincount` calculates the number of times each value occurs in an array of non-negative integers\
\
# The parameters `classes` and `y` denote the array of classes occurring in the data and the array of original class labels per sample respectively\
\
# Since float64 is not supported by the MPS framework, it needs to be converted to float32\
\
Configuring PyTorch Lightning Training Section\
\
# The parameter `monitors` indicates the quantity to monitor, the default value is None, i.e., only the checkpoint of the last epoch is saved\
\
# The parameter `save_top_k` can be set to any integer equal to or greater than -1\
\
# If the parameter `save_top_k` is set to k, it means that the best k models according to the quantity monitored will be saved; if it is set to 0, it means that no models will be saved; and if it is set to -1, it means that all models will be saved\
\
# The `mode` parameter should be one of the strings `min` and`max`, indicating that if `save_top_k` is not set to 0, the decision to overwrite the current save file is made based on either the maximization or the minimization of the monitored quantity\
\
Configuring Checkpoints for Model Logging\
\
# The `Trainer` function in the LightningModule can be used to create a model trainer by customizing every aspect of training by flags, which can be customized by the parameters`accelerator`, `devices`, `logger` and `callbacks` for the type of accumulator, the device used, the experiment tracking logger, the callback or the list of callbacks, respectively\
\
# In particular, the parameter `log_every_n_steps` indicates how often to log within steps, with a default value of 50, and the parameter `max_epochs` specifies a number of epochs, which the model trainer stops training once it is reached, with a default value of None\
\
# The `fit` function runs the full optimization routine for LightningModule's model trainer by setting the basic parameters `model`, `train_dataloaders` and `val_dataloaders`\
\
Creating and Implementing Model Trainer\
\
### Saving and Restoring the Trained Model\
\
# Use the `save_checkpoint` function to manually save the model's checkpoints through the model trainer so that the model can be restored later from the saved checkpoints\
\
Saving the Trained Model to a Specified Checkpoint\
\
# It is easy to load and instantiate the LightningModule directly from a checkpoint using the `load_from_checkpoint` function\
\
# The parameter `weight` in the `load_from_checkpoint` function of the Lightning module is inherited from the initial function `__init__` of the instantiated class`PneumoniaModel`, so it can be set according to the settings used in the model training\
\
# The parameter `strict` indicates whether it is strictly enforced that the key value in the checkpoint located by the parameter `checkpoint_path` matches the key value returned by the module's`state_dict`, which is set to True by default, although a setting of False is also acceptable in some special cases\
\
# Remember that it is mandatory to call the `eval` function of this model to set dropout and batch normalization layers to evaluation mode before running the inference; otherwise it will lead to inconsistent inference results\
\
Restoring the Trained Model from a Specified Checkpoint\
\
### Prediction and Evaluation of Validation Dataset\
\
# Use the `torch.unsqueeze` method to add the batch dimension before the other dimensions, as the trained model requires 4D inputs rather than 3D inputs\
\
# As with the model trainer, the `torch.sigmoid` function here is equivalent to manually adding the final softmax or sigmoid layer to the model that is imported from the `torchvision.models` module\
\
# Don't forget to remove the batch channel dimension as it is no longer needed\
\
# Using the `cpu` function of the PyTorch tensor object returns a copy of that object in CPU memory, but if the object is already in CPU memory and on the correct device, the copy is not executed, and the original object is returned\
\
Computing Predictions on the Complete Processed Validation Dataset\
\
# Confusion matrix provides insight into the correctness of the predictions and how well the predicted values match the actual values, one such confusion matrix for binary classification is a two-by-two table consisting of the number of the four outcomes of a binary classifier\
\
# The four outcomes obtained above through the binary classifier are TP, FN, TN and FP, which stand for true positive, false negative, true negative and false positive, respectively\
\
# As with the `Accuracy` class, other classes from the `torchmetrics` module, such as the `Precision` class, the `Recall` class, and the `ConfusionMatrix` class, also require the parameter`task`, which needs to be set to "binary", "multiclass" or "multilabel" to specify the type of task, and the parameter`threshold`, which is used to indicate a threshold for transforming probabilities into binary predictions, if this parameter is ignored, the default setting of 0.5 will be used\
\
# Normally, the parameter `num_classes` in the `ConfusionMatrix` class needs to be set to an integer to specify the number of labels, but when the parameter `task` is set to "binary", this parameter can be ignored and kept at the default value of None\
\
# Accuracy is one of the simplest of all machine learning metrics, it is the ratio of the total number of correct predictions to the total number of predictions, and intuitively serves as a defining metric for a machine learning model, but usually precision and recall are also suggested\
\
# Accuracy = (TP + TN) / (P + N) = (TP + TN) / (TP + FN + TN + FP)\
\
# Precision is the ratio between true positives and all positives, and may be more important in cases such as fraud detection, where it is better to avoid false positives and not accuse innocent people of fraud\
\
# Precision = TP / (TP + FP) = 1 - FP / (TP + FP)\
\
# Recall is a measure of how well the model correctly identifies true positives, and thus a high recall is usually more important in cases such as medical diagnostics, where it is better to identify all possible disease cases (even if there will be a few false positives) than to miss any\
\
# Recall = TP / P = TP / (TP + FN) = 1 - FN / (TP + FN) = 1 - FN / P\
\
# Due to data imbalances, a large number of false-negative results will appear in the processed training and validation datasets, and such a phenomenon is particularly serious in the field of medical imaging, as the missed results can be fatal, so to solve this problem, the model can be retrained with a new weighted loss, as mentioned earlier, or a simple alternative can be used, that is, to lower the classification thresholds, for example from 0.5 to 0.25, which reduces false negatives but increases the number of false positives, which is known as the precision-recall trade-off\
\
At a Threshold of 0.5\
\
Confusion Matrix:\
\
Validation Accuracy:\
\
Validation Precision:\
\
Validation Recall:\
\
Improving Model Evaluation through a Precision-Recall Trade-off Approach\
\
### Visualization of Prediction and Evaluation\
\
Confusion Matrix at a Threshold of 0.5\
\
Visual Comparison of Confusion Matrices at Different Thresholds\
\
Pneumonia Prediction by Batch of Randomized Images (Threshold 0.5)\
\
Visual Comparison of Pneumonia Prediction under Different Thresholds by Batch of Randomized Images\
\
### Class Activation Mapping (CAM)\
\
# `torch.nn.Module` provides the `children` method, which returns an iterator over the direct children of the module, through which it is possible to fetch all layers in the `resnet18` model, as well as select specific layers, for instance, selecting all layers before `avgpool` (average pooling)\
\
# The class `torch.nn.Sequential` is a sequential container to which the layers in the module will be added in the order passed in the constructor, or, alternatively, the module layers can be passed in as a dictionary with the parameter `OrderedDict`\
\
# Remarkably, the layers of modules in the `torch.nn.Sequential` class are connected in a cascading way, but since they are initially stored in a list, the list of layers here needs to be unpacked by using the `*` method\
\
Initial Understanding on Creating a PyTorch Sequence Model\
\
* The interpretability discussed here is the localization of the predicted focal areas of pneumonia, inspired by the paper "Learning Deep Features for Discriminative Localization."\
\
* This research paper introduces class activation mapping (CAM), a convolutional neural network (CNN) technique using global average pooling (GAP), which enables classification-trained CNNs to learn object localization without bounding-box annotations and to produce class activation maps that visualize the predicted class scores on images, highlighting discriminative object parts detected by the CNN, and whose accurate generation is essential for Weakly Supervised Semantic Segmentation (WSSS).\
\
* The main idea of CAM is to compute the softmax output $S^c_k$ of the class $c$ from the last convolutional layer (consisting of $k$ feature maps) before the fully connected (FC) layer by multiplying each corresponding feature map $F_k$ with the class weight $w^c_k$ of the subsequent FC layers.\
\
\
\
---\
\
This paper is publicly available at the following link: https://arxiv.org/pdf/1512.04150.pdf.\
\
# As stated in the inspired paper, despite the excellent ability of convolutional layers to localize objects, this ability is lost when classification is performed with FC layers, making it necessary to avoid FC layers and minimize the number of parameters while maintaining high performance\
\
# In addition to not extracting the original FC layer (i.e., the last layer) when extracting the feature map from the `resnet18` model, the original average pooling layer (i.e., the penultimate layer) is also not extracted\
\
# The `torch.nn.AdaptiveAvgPool2d` class applies 2D adaptive average pooling to an input signal consisting of multiple input planes\
\
# Note that for any input size, the output size of the `torch.nn.AdaptiveAvgPool2d` class is H x W, and the number of output features is equal to the number of input planes\
\
# The parameter `output_size` indicates the target output size of an image of the form H x W, which can be either a tuple (H, W) or a single H of a square image H x H\
\
# `torch.flatten` flattens the input by reshaping it into a 1D tensor\
\
# At this point, the average pooling layer and FC layer that were not extracted when the feature map was extracted have been filled in, and the FC layer can be used to compute the prediction\
\
# The main idea of CAM is to compute the class activation map M_k of the last convolutional layer (consisting of k feature maps) before the FC layer by multiplying each corresponding feature map F_k with the weight w_k of the subsequent FC layer, and to calculate the integrated class activation map M by summation, i.e., M = \uc0\u8721 _k w^c_k * F_k\
\
# In the above equation, F_k = \uc0\u8721 _ij f_ij, where f_ij represents the value of each spatial location of F_k, therefore, the above equation can be rewritten as, M = \u8721 _k \u8721 _ij w^c_k * f_ij\
\
# Consider all feature maps denoted by A_k as a whole feature map A, and all weights denoted by w_k as a whole weight w, so the computation of the class activation map M can be simplified to computing the dot product of the weight w and feature map A, i.e., M = w \'95 A\
\
# In this example, the shape of the returned feature tensor is 512x7x7, which needs to be reshaped to a tensor of shape\'a0512x49 in order to simplify the multiplication operation\
\
# Only weights are needed to compute the CAM, not bias\
\
# The `detach` function returns a new tensor separated from the current graph, which is used to remove the gradient information from the weight parameters for numpy conversion and never needs the gradient\
\
# Use `torch.matmul` to compute the dot product between weight w and the earlier mentioned feature map A, as it will directly multiply each feature map A_k with the weight w_k of the subsequent FC layer and compute the sum of the products of all feature maps\
\
# In this example, a 49-element vector (7x7 elements) will be yielded here\
\
# Normalize the dot product generated class activation map (min-max feature scaling is used here), but this is not always necessary\
\
# Reshape the normalized class activation map to the original shape of the feature tensor (512x7x7 here) and move the tensor back to the CPU\
\
# Although the construction of the new model has changed, the structure of all neural layers (including the final global average pooling layer as well as the fully connected layers) and the forward/backward propagation passes remain unchanged, so that weights previously trained and stored in the former model can be used directly\
\
# The reason why the parameter `strict` needs to be set to False is that the structure of the new model has changed with the addition of the CAM-related feature maps, but since there were no CAM-related feature maps at all when the previous model was trained, the previous model would not have stored explicit weights for these variables, as well as the new model would not have used the loss function of the previous model\
\
# Here, the `eval` function of this model must also be called to set the dropout and batch normalization layers to evaluation mode so as to avoid inconsistent results in inference\
\
Creating a New Model for Implementing Class Activation Mapping (CAM)\
\
### Visualization of CAM\
\
# The attribute `samples` of the class `DatasetFolder` is a list of tuples, with each tuple containing the file path of each sample and the index of the class it belongs to\
\
# The attribute `classes` of the class `DatasetFolder` is a list of the class names\
\
original X-ray image\
\
# The `torchvision.transforms.functional.resize` resizes the input image to the given size, and if the image is a torch tensor, it is expected to have the shape[... , H, W], where ... denotes an arbitrary number of leading dimensions\
\
# The parameter `img` indicates the image to be resized, the type can be a PIL image or a tensor\
\
# The parameter `size` indicates the desired output size, if it is a sequence such as (h, w), the output size will be matched; if it is an int, the smaller edges of the image will be matched to maintain the aspect ratio\
\
# The parameter `antialias` is set to True by default, which means that antialiasing will be applied to bilinear or bicubic modes, but other modes are not affected\
\
X-ray image with CAM extracted from the 2nd residual block of the 5th convolutional layer in the ResNet-18 architecture\
\
# `axes.get_position` returns the position of the Axes in the figure as a bbox, where the parameter `original` is set to False by default, which means the active position is returned; otherwise the original position is returned\
\
# `matplotlib.figure` implements the following classes: the class`matplotlib.figure.Figure`, the top-level artist and contains all the plotting elements, many methods are implemented in the class`matplotlib.figure.FigureBase`; the class`matplotlib.figure.SubFigure`, the logical figure in the figure, and the class`matplotlib.figure.SubplotParams`, is used to control the default spacing between subfigures\
\
# The class `matplotlib.figure.Figure` provides the `Figure.colorbar` method for adding a color bar to a plot\
\
# The parameter `mappable` represents the `matplotlib.cm.ScalarMappable` object (usually an image) that indicates the colormap and the norm to be used, which is required for the `Figure.colorbar` method, but optional for the `pyplot.colorbar` function, which will be set by default to the current image\
\
# The parameter `cax` is an optional parameter that indicates the Axes on which the color bar is to be drawn, or, if None (the default), creates a new Axes whose space will be stolen from the Axes(s) specified in the parameter `ax`\
\
# The parameter `ax` is an optional parameter that specifies one or more parent Axes from which the new colorbar Axes will take its space, which is only used if the parameter `cax` is not set, and whose default value is None\
\
Visual Comparison of Original X-Ray Images and CAM-Assisted Images of 10 Randomized True-Positive Cases (Threshold 0.5) for Pneumonia Prediction\
\
Pneumonia Prediction by Batch of CAM-Assisted Randomized Images (Threshold 0.5)\
\
Visual Comparison of Pneumonia Prediction under Different Thresholds by Batch of CAM-Assisted Randomized Images\
\
# Beyond the Lecture\
\
This part has been added to self-implement other more advanced CAM methods using techniques outside of the lecture, including Grad-CAM, Grad-CAM++, and Score-CAM.\
\
## Understanding the PyTorch Hooks\
\
This section was inspired by the tutorial "PyTorch Hooks Explained - In-depth Tutorial" posted on [Elliot Waite's YouTube channel](https://www.youtube.com/@elliotwaite).\
\
### Recap of the Computational Graph\
\
# In the automatic differentiation engine `torch.autograd` of PyTorch, if the parameter `requires_grad` is set to True, the input tensor for the operation will be tracked for computation, meaning that the gradient associated with that tensor will be accumulated in the `.grad` attribute after the computation is passed backward\
\
# The `backward` function provided by the PyTorch tensor computes the gradient of the current tensor with respect to the graph leaves, where the parameter `gradient` denotes the gradient with respect to the tensor, which is passed to start the backward pass, with a default value of None, meaning that it starts with the default gradient value (a tensor value of 1)\
\
Understanding How Computational Graphs Work through a Simple Example\
\
# The `backward` function of the PyTorch tensor is called only once during the backward pass of the computational graph, which results in a call to the `torch.autograd.backward` function for that tensor and differentiates the graph according to the chain rule\
\
Understanding How Computational Graphs Work through a Slightly More Complex Example\
\
### Settings for Tensor Hooks\
\
# The gradient in the hook should avoid being modified by in-place operations, but there is an option to return a new gradient that can be used to replace the old one\
\
# Once the `backward` function of the PyTorch tensor is called, the gradients through non-leaf nodes will be inaccessible throughout the computation, and the gradients cannot be inspected or modified as they flow through the nodes, but can only be checked by outputting them to the leaf nodes\
\
# This is where the hooks on the tensor need to come into play, allowing the user to backward check the gradient as it flows through the graph, and change the gradient as needed\
\
# The `register_hook` function provided by the PyTorch tensor registers a backward hook that is called each time the gradient with respect to the tensor is computed, and it should have the following signature: `hook(grad) -> Tensor or None`\
\
# The parameter `hook` in the `register_hook` function of the PyTorch tensor can be passed by either a regular function or a lambda function, which returns either the new gradient or the same gradient (if the gradient hasn't changed)\
\
# Note that registered hooks will be added to the ordered dictionary, and the order in which the hooks are added to the tensor is very important, determining the order that hooks will be called in the backward flow of the graph\
\
# The `retain_grad` function provided by the PyTorch tensor enables the tensor to populate the gradient during backward flow, while the leaf tensor disables this option\
\
# Note that the hook that retains the gradient is also added to the ordered dictionary, but the order is no longer important; it can be added before or after the other hooks and only needs to be added once\
\
Understanding How Hooks Work on Tensors through a Simple Example\
\
# The `register_hook` function of the PyTorch tensor can return a handle on which to provide the `handle.remove` method to remove the hook from the module\
\
Understanding How to Use Hook Handles on Tensors through a Simple Example\
\
### Setting of Module Hooks\
\
# The `register_module_forward_pre_hook` function of the PyTorch module registers a forward pre-hook that is common to all modules, which will be called before each call to the model's `forward` function, with the following signature: `hook(module, input) -> None or modified input`\
\
# The `register_module_forward_hook` function of the PyTorch module registers a generic forward hook for all modules, which will be called after each call to the model's `forward` function, with the following signature: `hook(module, input, output) -> None or modified output`\
\
# As practice has shown, the two forward hook register functions of the PyTorch module do not automatically retain the change of the non-endpoint nodes during the forward process; these nodes will return to their original values after the computation\
\
Understanding How to Set up Different Forward Hooks on a Module through a Simple Example\
\
# The `register_full_backward_pre_hook` function of the PyTorch module registers a backward pre-hook for the module, which is called every time the module's gradient is computed, with the following signature: `hook(module, grad_output) -> tuple[Tensor] or None`\
\
# The `register_module_full_backward_hook` function of the PyTorch module registers a generic backward hook for all modules, which is called every time the module's gradient is computed, i.e., the hook is executed only if the gradient of the module's output is computed, with the following signature: `hook(module, grad_input, grad_output) -> Tensor or None`\
\
# Similarly, it has been shown in practice that the two backward hook register functions of the PyTorch module do not automatically retain the gradient change of non-leaf nodes during the backward process, and that these nodes will return to their original gradients after the computation\
\
# Note that the `register_hook` function of the PyTorch tensor here will only record the original gradient, but will not retain the change of gradient; if it is not called, the node will just return None as the gradient\
\
Understanding How to Set up Different Backward Hooks on a Module through a Simple Example\
\
# Using the backward hook register function of the PyTorch tensor inside the module can be used as an alternative to the two backward hook register functions of the PyTorch module to achieve the effect of changing the gradient of the internal nodes, and not only that, it retains the change of gradient\
\
Understanding How to Set up Tensor Backward Hooks on a Module through a Simple Example\
\
\
\
# Here is a good demonstration of why invoking the `model.__call__` function shorthanded as `model` is better than invoking the `model.forward` function directly, since the latter and all hooks are dispatched in the former\
\
# So if the `forward` function and any hooks exist in the model, it is better to invoke the `__call__` function directly\
\
\
Creating a New Model for Both Implementing CAM and Gradient-Weighted Class Activation Mapping (Grad-CAM)\
\
\
Creating a New Model for Implementing Grad-CAM with More Flexibility in Terms of Structure\
\
\
Visual Comparison of Original X-Ray Images and Grad-CAM-Assisted Images of 10 Randomized True-Positive Cases (Threshold 0.5) for Pneumonia Prediction\
\
\
\
\
Visual Comparison of X-Ray Images for Pneumonia Prediction Assisted by Grad-CAM Extracted from Different Convolutional Layers and Residual Blocks in the ResNet-18 Architecture\
\
\
\
\
\
Pneumonia Prediction by Batch of Grad-CAM-Assisted Randomized Images (Threshold 0.5)\
\
Visual Comparison of Pneumonia Prediction under Different Thresholds by Batch of Grad-CAM-Assisted Randomized Images\
\
\
\
\
####\
\
\
\
# The models in the `torchvision.models` module are trained by using`nn.CrossEntropyLoss`, which consists of `nn.LogSoftmax` and`nn.NLLLoss`, that is why each model ends up without a softmax layer\
}
# General Information

![Course Title](../Images/Course_Title.jpeg)

**Course Tags:** #Python, #PyTorch, #machine learning, #deep learning, #medical image analysis

**Section Tags:** #PyTorch Lightning, #Pydicom, #convolutional neural network, #CNN, #residual learning, #residual network, #ResNet-18, #image classification, #object localization, #discriminative regions, #class activation mapping, #CAM, #gradient-weighted class activation mapping, #Grad-CAM, #Grad-CAM++, #chest X-ray images, #pneumonia

**Author:** [Hao ZHANG](https://www.linkedin.com/in/hao-zhang-6b7008107/)

**Description**

This notebook organizes the content of “Section 8: Pneumonia-Classification” from the Udemy course “Deep Learning with PyTorch for Medical Image Analysis”$^{\sf_{1}}$ and makes modifications or additions where necessary. As far as possible, while adapting to the current versions of the various necessary libraries and to the author's own computer's system environment, in addition to completing the steps in the lecture, additional annotations as well as relevant references have been added, together with more informative visualization performances and visualization comparisons.

The chest X-ray image dataset used for training in this lecture was downloaded from a Kaggle Competition.$^{\sf_{2, 3}}$ This notebook demonstrates how to train chest X-ray images for pneumonia and non-pneumonia classification using the ResNet-18 model, a deep residual learning model for image recognition,$^{\sf_{4}}$ and compares the accuracy, precision, and other metrics of this classification training at different thresholds. This classification training was performed using a deep learning algorithm trained on a large chest X-ray image dataset to recognize patterns and features indicative of pneumonia, which involves the use of convolutional neural networks (CNNs). Incidentally, the CNN architecture is a deep learning architecture that is particularly suited for image classification tasks, while the Residual Network (ResNet) model utilizes the CNN architecture and provides a solution to the vanishing gradient problem common to this architecture.

The framework of CNN is trained to perform classification on medical image datasets, but it can also be used to identify focal regions in medical images corresponding to infected regions (e.g., pneumonia) by learning image features in the convolutional layer, i.e., identifying discriminative regions. This image-level supervised object localization method is a weakly supervised object localization (WSOL) method, and most existing WSOL methods rely on various thresholding class activation mappings (CAMs) generated by pre-trained networks to highlight and localize objects. Therefore this notebook also highlights and localizes objects by generating class activation maps from the trained ResNet-18 model, these methods for generating class activation maps include (CAM, gradient-weighted class activation mapping (Grad-CAM), Grad-CAM++, etc.).$^{\sf_{5 - 7}}$

**References**

1. [Deep Learning with PyTorch for Medical Image Analysis](https://www.udemy.com/course/deep-learning-with-pytorch-for-medical-image-analysis/)

2. [RSNA Pneumonia Detection Challenge](https://www.kaggle.com/competitions/rsna-pneumonia-detection-challenge/)

3. [X. Wang, Y. Peng, L. Lu, Z. Lu, M. Bagheri and R. M. Summers, "ChestX-Ray8: Hospital-Scale Chest X-Ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases," 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Honolulu, HI, USA, 2017, pp. 3462-3471, doi: 10.1109/CVPR.2017.369.](https://arxiv.org/abs/1705.02315)

4. [K. He, X. Zhang, S. Ren and J. Sun, "Deep Residual Learning for Image Recognition," 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV, USA, 2016, pp. 770-778, doi: 10.1109/CVPR.2016.90.](https://arxiv.org/abs/1512.03385)

5. [B. Zhou, A. Khosla, A. Lapedriza, A. Oliva and A. Torralba, "Learning Deep Features for Discriminative Localization," 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV, USA, 2016, pp. 2921-2929, doi: 10.1109/CVPR.2016.319.](https://arxiv.org/abs/1512.04150)

6. [R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh and D. Batra, "Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization," 2017 IEEE International Conference on Computer Vision (ICCV), Venice, Italy, 2017, pp. 618-626, doi: 10.1109/ICCV.2017.74.](https://arxiv.org/abs/1610.02391)

7. [A. Chattopadhay, A. Sarkar, P. Howlader and V. N. Balasubramanian, "Grad-CAM++: Generalized Gradient-Based Visual Explanations for Deep Convolutional Networks," 2018 IEEE Winter Conference on Applications of Computer Vision (WACV), Lake Tahoe, NV, USA, 2018, pp. 839-847, doi: 10.1109/WACV.2018.00097.](https://arxiv.org/abs/1710.11063)

***

# Importing Libraries

## Theme-Related Libraries

***PyTorch Lightning***

- *PyTorch Lightning* is an open source Python library that provides a high-level interface to PyTorch, a popular deep learning framework for professional artificial intelligence researchers and machine learning engineers.

- The `ModelCheckpoint` class from the `lightning.pytorch.callbacks` module: periodically saves the model by monitoring a quantity, where each metric recorded as a key-value pair or value dictionary is a candidate for the monitored key.

- The `TensorBoardLogger` class from the `lightning.pytorch.loggers` module: logs to local or remote file system in TensorBoard format, for more information on TensorBoard, click [here](https://www.tensorflow.org/tensorboard).

> **Tips**
>
> - TensorBoard can be started either from the command line or directly from the notebook by experiencing the magic commands provided by the IPython kernel. The two commands are roughly the same, except that in the notebook, the IPython kernel uses `%` as a syntax element for magic commands, i.e., the line magic starting with `%tensorboard`. On the command line, run the same command without `%`.
>
> - Before using the line magic `%tensorboard`, the TensorBoard notebook extension needs to be loaded using the magic command `%load_ext tensorboard`.
>
> - To start TensorBoard, the previously used root log directory must be specified. The argument `logdir` is used to point to the directory where the root directory structure is located, and then TensorBoard will recursively traverse this directory and find the event files to display. The command line to start TensorBoard is: `tensorboard --logdir=path_to_logs`. Of course, the magic command to start TensorBoard is: `%tensorboard --logdir=path_to_logs`.

***TorchMetrics***

- *TorchMetrics* is a collection of over 100 PyTorch metric implementations and an easy-to-use API for creating custom metrics. Among other benefits, it provides a standardized interface for improved repeatability and automatic batch accumulation.

- *TorchMetrics* was originally created as part of PyTorch Lightning and can be used in any PyTorch model, as well as in PyTorch Lightning to enjoy additional benefits such as the ability to log `Metric` objects directly in Lightning in order to reduce even more boilerplate.

**The *Torchvision* Library**

- The *torchvision* library is part of the PyTorch project and consists of popular datasets, model architectures, and common image transformations for computer vision.

- The `torchvision.datasets` module: contains numerous built-in datasets, as well as utility classes for building users' own datasets.

- The `torchvision.models` package: contains definitions of models to handle different tasks including image classification, object detection, and so forth.

- The `torchvision.transforms` module: contains commonly used computer vision transformations that can be used to transform or augment data for training or inference on different tasks (image classification, detection, and so on).

- The `torchvision.utils` module: contains various utilities, mainly for visualization. Among them, the `make_grid` function is used to make image grids.

**The `torch.utils.data` API**

- The `torch.utils.data` API is a PyTorch data loading utility centered around the `DataLoader` class.

- The `torch.utils.data.Dataset` class: an abstract class that represents a dataset.

- The `torch.utils.data.DataLoader` class: a data loader that combines a dataset with a sampler and provides an iterable over the given dataset.

- The `torch.utils.data.RandomSampler` class: a sampler that samples elements randomly. If no replacement is performed, samples elements from a shuffled dataset.

**The `sklearn.utils` Module**

- Scikit-learn contains numerous utilities that assist in development, these are located in the `sklearn.utils` module and include several categories of tools.

- The `sklearn.utils.class_weight` module: contains utilities for handling weights based on class labels, including the `compute_class_weight` function used to estimate class weights for imbalanced datasets.

***

## Visualization-Related Libraries

***

## Other Auxiliary Libraries

***

# Configuring Settings

## Visualization-Related Settings

***

## Other Auxiliary Settings

***

# Pre-installing Custom Modules

***

# Implementation of Lecture Content

## Preprocessing

### Preprocessing of the Original Training Dataset Labels

**Introduction to the Dataset**

- Pneumonia is a serious disease that kills more than 15% of children under 5 years old worldwide, is a common cause of emergency department visits, and is the leading cause of death in the United States. Diagnosing pneumonia is difficult and requires reviewing chest X-ray images, considering other lung issues and technical factors.

- The [Radiological Society of North America (RSNA®)](https://www.rsna.org/) is an international society comprised of radiologists, medical physicists, and other medical professionals from around the world who work with medical imaging, such as X-ray images. The RSNA has reached out to [Kaggle](https://www.kaggle.com/)’s machine learning community and collaborated with other partners in organizing the [RSNA Pneumonia Detection Challenge](https://www.kaggle.com/competitions/rsna-pneumonia-detection-challenge/) and developing a rich X-ray image dataset for this challenge.

- The medical image dataset used in this lecture was downloaded from the aforementioned Kaggle Competition, which was originally sourced from the [National Institutes of Health (NIH) Clinical Center](https://clinicalcenter.nih.gov/)'s publicly available [chest X-ray image dataset](https://nihcc.app.box.com/v/ChestXray-NIHCC). The related [NIH](https://www.nih.gov/) press release is available at the following [link](https://www.nih.gov/news-events/news-releases/nih-clinical-center-provides-one-largest-publicly-available-chest-x-ray-datasets-scientific-community).

> **Bibliographies**
>
> - [RSNA Pneumonia Detection Challenge](https://www.kaggle.com/competitions/rsna-pneumonia-detection-challenge/)
>
> - [X. Wang, Y. Peng, L. Lu, Z. Lu, M. Bagheri and R. M. Summers, "ChestX-Ray8: Hospital-Scale Chest X-Ray Database and Benchmarks on Weakly-Supervised Classification and Localization of Common Thorax Diseases," 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Honolulu, HI, USA, 2017, pp. 3462-3471, doi: 10.1109/CVPR.2017.369.](https://arxiv.org/abs/1705.02315)

***

```
# Positive targets indicated that these subjects suffered from pneumonia, with more positive targets indicating a more significant condition.
```

Obtaining Labels from the Original Training Dataset

***

```
# The `drop_duplicates` function from the `pandas.DataFrame` class: returns the DataFrame from which duplicate rows have been removed.
```

Removing Duplicate Labels from the Original Training Dataset

***

```
# The `with_suffix` function from the `pathlib.PurePath` class returns a new path with the suffix changed. Note that if the original path has no suffix, the specified argument `suffix` is appended instead; if the specified argument `suffix` is an empty string, the original suffix is removed.
```

Subject with {targets} Diagnostic Targets

Subject without Any Diagnostic Targets

Visual Comparison of Subjects with the Most Pneumonia Diagnostic Targets to Randomized Subjects without Any Pneumonia Diagnostic Targets

***

### Preprocessing of DICOM Files in the Original Training Dataset

**Digital Imaging and Communications in Medicine (*DICOM*)**

- Digital Imaging and Communications in Medicine (*DICOM*) is a standard for the storing and transferring medical images and related information that specifies a non-proprietary data exchange protocol designed to facilitate the transmission of images between machines from different manufacturers.

- Bits Stored (0028,0101) Attribute: the number of bits stored for each pixel sample. Note that the number of bits stored for each sample must be the same.

***

```
# Resize the original image to 224 x 224 to reduce the processing load on the deep learning model. Meanwhile, when storing the image, the image is converted to the half-precision data type to reduce the amount of space used. Because in computing, half-precision (sometimes referred to as FP16 or float16) is a binary floating-point computer number format that occupies two bytes in computer memory and is used to represent numbers with small storage space and low precision.

# Retrieve the corresponding label for each image.

# Split the original training dataset into a processed training dataset and a processed validation dataset.

# Divide each of the two freshly split datasets into labeled subsets based on their labels.

# The `mkdir` function from the `pathlib.Path` class creates a new directory at the given path. If the argument `parents` is False (the default is False), the `FileNotFoundError` exception is raised if the parent directory is missing; otherwise, it creates the missing parent directory under the path as needed. As well, if the argument `exist_ok` is False (the default is False), it raises the `FileExistsError` exception in case the given path already exists on the file system; otherwise it is ignored if the given path already exists and is not a directory.
```

Preprocessing and Splitting the Original Training Dataset

***

```
# Calculate dataset statistics using only the processed training dataset.

# The`numpy.load` function can load arrays or pickled objects from `.npy`, `.npz`, or pickle files.
```

Statistical Computing on the Processed Training Dataset

***

## Train - Part One - Data Loading

### Data Transformation

**The `torchvision.transforms` Module**

- The `torchvision.transforms` module in the torchvision library supports common computer vision transformations i.e., transforming or augmenting data for training or inference on different tasks (image classification, detection, segmentation, video classification).

> **Tips**
>
> - In order to make reliable predictions, deep learning models usually require a large amount of training data, which is not always available, and thus require augmentation of existing data to build better generalized models.
>
> - Data augmentation is a technique to increase the amount of data used to train a model.
>
> - The most commonly used image data augmentation techniques include *positional augmentation* (e.g., scaling, cropping, flipping, padding, rotation, translation, affine transformation) and *color augmentation* (e.g., brightness, contrast, saturation, hue).

- The `torchvision.transforms.Compose` class: composes several transforms together.

  - The argument `transforms`: the list of transforms to compose.

- The `torchvision.transforms.Normalize` class: normalizes the tensor image with mean and standard deviation.

  - The argument `inplace`: indicates whether used to make this operation in-place, defaults to False.

- The `torchvision.transforms.RandomAffine` class: sets up a random affine transformation of the image, by keeping the center invariant. Note that if the image is a PyTorch tensor, its shape should be [... , H, W], where ... denotes an arbitrary number of leading dimensions.

  - The argument `degrees`: the range of degrees to be selected. Note that if it is a number and not a sequence like (min, max), the range of degrees will be between its positive and negative absolute values, if it is set to 0, it means that the rotation is deactivated.
  
  - The argument `translate`: the maximum absolute fraction tuple for horizontal and vertical translation.
  
  - The argument `scale`: the scaling factor interval.

- The `torchvision.transforms.RandomResizedCrop` class: crops a random portion of the image and resizes it to a given size.

  - The argument `size`: the expected size of the cropping output for each edge.
  
  - The argument `scale`: specifies the upper and lower bounds for cropping a random area before resizing.
  
  - The argument `ratio`: specifies the upper and lower bounds of the random aspect ratio for cropping before resizing.
  
  - The argument `antialias`: indicates whether antialiasing is applied. Note that it only affects tensors with bilinear or bicubic modes; otherwise it is ignored.

***

```
# Data augmentation is only required for the processed training dataset, not the processed validation dataset.
```

Defining Pipelines for Data Transformation and Data Augmentation

***

**The `torchvision.datasets` Module**

- The `torchvision.datasets` module in the *torchvision* library provides several built-in datasets, along with utility classes for constructing users' own datasets.

- The `torchvision.datasets.DatasetFolder` class: a generic dataset loader with a custom root path and a custom function to load samples for the given path, which are set by the arguments `root` and `loader`.

  - The argument `root`: the root directory path.
  
  - The argument `loader`: the function to load a sample given its path.
  
  - The argument `extensions`: the list of allowed extensions.
  
  - The argument `transform`: the function/transformation that takes the sample and returns the transformed version.

***

```
# Since the data was previously stored as float16 to reduce space, a reconversion to float32 is required to improve calculation accuracy.

# The `np.unique` function returns the sorted unique elements of the array, where the argument `ar` denotes the input array, and if the optional argument `return_counts` is True, it also returns the number of times each unique item appears in the input array.
```

Loading the Processed Training Dataset and the Processed Validation Dataset

***

```
# Create a new class for each dataset and add indexes so that the data can be retrieved later by the indexes.
```

Labeled Image Grid of the Processed {dataset} Dataset Samples

```
# The `torch.cat` function concatenates the given sequence tensor by a given dimension. The argument `dim` indicates the dimension over which the tensors are concatenated.
```

Visual Comparison of Labeled Image Grids for Two Processed Dataset Samples

***

### Batch Loading

**The `torch.utils.data.DataLoader` Class**

- The `torch.utils.data.DataLoader` class: a data loader that combines a dataset with a sampler and provides an iterable over a given dataset. Note that this class supports mapped and iterable datasets, single- or multi-process loading, custom loading order, and optional automatic batching (collation) and memory pinning.

- The argument `batch_size`: indicates how many samples to load per batch, default value is 1.

- The argument `shuffle`: indicates whether the data is reshuffled at every epoch, the default value is False.

- The argument `num_workers`: indicates how many child processes will be used to load the data, the default value is 0.

> **Tips**
>
> - When the argument `num_workers` is set to 0 (i.e., the default value), it means that the data will be loaded in the main process.
>
> - When the argument `num_workers` is set to greater than 0, PyTorch will use multiple processes to load the data, but the Jupyter Notebook (IPython kernel) does not support Python multiprocessing.

***

```
# Since Jupyter Notebook does not support Python multiprocessing, the argument `num_workers` can only be set to 0 here.
```

Configuring Batch Loading

***

Labeled Image Grid of the First Batch of Data Extracted from the Processed Training Dataset

Labeled Image Grid of the First Batch of Data Extracted from the Processed Validation Dataset

Visual Comparison of Labeled Image Grids for the First Batch of Data Extracted from Two Processed Datasets

***

## Train - Part Two - Model Creation

***Residual Learning* and *Identity Mapping***

- Deep convolutional neural networks significantly improve image classification by using multiple layers to process different levels of features. The effectiveness of deep convolutional neural networks depends on their depth, but past experiments have shown that networks with deeper layers are not really better.

- Recent evidence demonstrates that the first problem faced by deep networks is the vanishing/exploding gradients, which can make it difficult for the network to converge. However, this issue is also effectively addressed through normalized initialization and intermediate normalization layers, allowing deep networks to converge.

- As deeper networks are able to start converging, deeper networks expose a second problem, degradation: the increase in depth of the network in turn decreases the accuracy of the network. However, this degradation problem is not caused by overfitting, because as the depth of the network increases, not only does the accuracy degrade rapidly after reaching saturation, but the training error of the model also increases.

- The degradation in training accuracy reveals that not all systems are similarly easy to optimize. In fact, for the deeper model, there exists a constructive solution: the added layers are *identity mapping*, while the other layers are copied from the shallower model that was learned. The existence of this constructed solution suggests that deeper models should not produce higher training errors than their shallower counterparts.

- Therefore, the deep *residual learning* framework is proposed to address this degradation problem.

  - The goal is not to directly let every few stacked layers fit the desired underlying mapping, but to explicitly let these layers fit a residual mapping: assuming that the desired underlying mapping is $\mathcal{H}(\mathrm{x})$, the residual mapping to be fitted by the stacked nonlinear layers is $\mathcal{F}(\mathrm{x}) \coloneqq \mathcal{H}(\mathrm{x}) - \mathrm{x}$. Thus, the original mapping is reformulated as $\mathcal{F}(\mathrm{x}) + \mathrm{x}$.
  
  - As mentioned earlier, if the added layers can be constructed as *identity mappings*, the training error of a deeper model should not be larger than that of its corresponding shallow model. With the *residual learning* recasts, if *identity mappings* are optimal, the solver can approximate *identity mappings* by simply setting the weights of multiple nonlinear layers (i.e., residual mappings) toward zero, which is much easier than fitting an *identity mapping* by a stack of nonlinear layers.
  
  - In reality, *identity mappings* are unlikely to be optimal, and the *residual learning* reformulation can be helpful for preconditioning this problem. If the optimal function approaches an *identity mapping* more closely than a zero mapping, it is much easier for the solver to find perturbations with respect to an *identity mapping* than it is to learn a new function. Experiments show that the learned residual functions are generally smaller in response, suggesting that *identity mappings* provide reasonable preconditioning.
  
  - The operation of the formula $\mathcal{F}(\mathrm{x}) + \mathrm{x}$ can be realized by feedforward neural networks with "shortcut connections" (i.e., connections that skip one or more layers) and element-wise addition.

    - In this *residual learning* framework, the shortcut connections simply perform *identity mapping*, then add their outputs to the outputs of the stacked layers, and adopt a second nonlinearity after the addition, as shown below.

     ![Shortcut Connections](../Images/Shortcut_Connections.png)

    - Since identity shortcut connections add no extra parameters or computational complexity, it is possible to make fair comparisons between plain and residual networks with the same parameters, depth, width, and computational cost (except for negligible element-wise additions). This not only applicable to fully-connected layers, but also to convolutional layers.

    - The entire network can still be trained end-to-end using SGD and backpropagation, and can be easily implemented using common libraries without modifying the solvers.

**Plain Network and *Residual Network***

- The baselines of plain networks are mainly inspired by the philosophy of VGG networks. The convolutional layers mostly use $3 \times 3$ filters, which are directly downsampled by a convolutional layer with a stride of 2, and follow two simple design rules:

  1. Feature map size equal: the number of filters in each layer is the same;
  
  2. Feature map size halved: the number of filters in each layer is doubled to maintain the time complexity.

- Inserting shortcut connections on the above plain network enables it to be converted to its counterpart residual version, i.e., the *residual network*.

  - When the dimensions of the input and output feature maps are the same, the identity shortcuts can be used directly.
  
  - When the dimensions of the output feature maps increase compared to the dimensions of the counterpart input feature maps, the following two options can be considered, both of which perform a stride of 2 when the shortcuts go through feature maps of two sizes:

    1. Zero-padding shortcuts: shortcuts with this option still perform identity mapping, with extra zero entries padded for increasing dimensions, and this option introduces no extra parameter;

    2. Projection shortcuts: shortcuts with this option match dimensions by a $1 \times 1$ convolution.

- The detailed network architecture of the plain and *residual networks* is shown below. For ease of representation, the 18/34/50/101/152-layer *residual network* (ResNet) could be abbreviated as ResNet-18/34/50/101/152, respectively.

 ![Network Architecture](../Images/Network_Architecture.png)

- It is worth noting that compared to the model of VGG networks (with the VGG-16/19 model as a reference), the model of 18/34/50/101/152-layer plain and *residual networks* and  has fewer filters and lower complexity.

> **Bibliographies**
>
> - [K. He, X. Zhang, S. Ren and J. Sun, "Deep Residual Learning for Image Recognition," 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), Las Vegas, NV, USA, 2016, pp. 770-778, doi: 10.1109/CVPR.2016.90.](https://arxiv.org/abs/1512.03385)

***

```
# Torchvision provides a new backwards compatible API for building models with multi-weight support, allowing different weights to be loaded in existing model builder methods, in which the argument `weights` defines the pre-training weights to be used, with the default value of None, i.e., no pre-training weights are used.
```

Initial Understanding of the ResNet-18 Architecture

***

**The `lightning.pytorch.LightningModule` API**

- The `lightning.pytorch.LightningModule` API is one of the two core APIs for PyTorch Lightning, and it organizes PyTorch code into 6 sections:

  1. Initialization (the `__init__` and `setup` functions);
  
  2. Train loop (the `training_step`functions);
  
  3. Validation loop (the `validation_step` functions);
  
  4. Test loop (the `test_step` functions);
  
  5. Prediction loop (the `predict_step` functions);
  
  6. Optimizers and learning rate schedulers (the `configure_optimizers` functions).

- The `lightning.pytorch.LightningModule` API is equivalent to the `torch.nn.Module` class, but with added functionality and many convenient methods.

**The `torch.nn.BCEWithLogitsLoss` Class**

- The `torch.nn.BCEWithLogitsLoss` class combines the `torch.nn.Sigmoid` class with the `torch.nn.BCELoss` class in a single class.

  - The `torch.nn.Sigmoid` class: applies the element-wise logistic sigmoid function.
  
    - The sigmoid function is any mathematical function whose graph has a characteristic S-shaped curve or sigmoid curve. There are a number of common sigmoid functions, such as the logistic function, the hyperbolic tangent, and the arctangent.

    - In some fields, notably artificial neural networks, the sigmoid function is often used to refer specifically to the logistic sigmoid function, which is a special form of the logistic function and a common example of the sigmoid function, usually denoted by $\sigma(x)$.

    - The logistic sigmoid function converts any real number in the domain $(-\infty, \infty)$ to a number between the codomain $(0, 1)$, and it is often useful in predicting probabilities and solving binary classification problems. More precisely, the logistic sigmoid function asymptotes at $\displaystyle{\lim_{x \to \infty} \sigma(x) = 1}$ and $\displaystyle{\lim_{x \to -\infty} \sigma(x) = 0}$, and crosses zero at $\sigma(x) = 0.5$. The formula is deduced as follows.

    $$\sigma(x) = \frac{1}{1 + e^{-x}} = \frac{e^{x}}{1 + e^{x}} = 1 - \frac{1}{1 + e^{x}} = 1 - \sigma(-x)$$

    - The derivative of the logistic sigmoid function is symmetric with respect to the y-axis and has a maximum at $\frac{d}{dx} \sigma(0) = 0.25$, with a codomain of $(0, 0.25]$. The derivative formula is deduced as follows.

    $$\frac{d}{dx} \sigma(x) = \frac{d}{dx} \left( \frac{1}{1 + e^{-x}} \right) = -\frac{1}{(1 + e^{-x})^2} \cdot \frac{d}{dx} (1 + e^{-x}) = -\frac{1}{(1 + e^{-x})^2} \cdot e^{-x} \cdot \frac{d}{dx} (-x) = \frac{e^{-x}}{(1 + e^{-x})^2} = \frac{1}{1 + e^{-x}} \cdot \left( 1 - \frac{1}{1 + e^{-x}} \right) = \sigma(x)(1 - \sigma(x))$$

    - There are two common problems with the logistic sigmoid function:

    1. The most common problem with the logistic sigmoid function is the vanishing gradient problem, which is commonly seen in multilayer neural networks that use logistic sigmoid functions. Since the derivatives of logistic sigmoid functions are very small, the derivatives become smaller and smaller when they are multiplied together during backpropagation, i.e., the smaller the gradients, the less effective the backpropagation will be.

    2. In addition, the logistic sigmoid function an activation function not zero-centered, which usually makes training a neural network more difficult and unstable. Therefore, for a neural network, a zero-centered activation function is preferred as it ensures that the average activation value is around zero, which helps in smoother and faster convergence during the training process.

   > **Tips**
 >
   > - The hyperbolic tangent (tanh) function is very similar to the logistic sigmoid function, although it has its own drawbacks. As an improved version of the logistic sigmoid function, it is commonly used as the activation function in the hidden layers of a recurrent neural network (RNN) which helps capture complex temporal dependencies and can handle both positive and negative values in sequential data, while the latter is often used in the output layer of a binary classification neural network. The function and derivative graphs of these two functions are shown below.
   >
   > ![Logistic Sigmoid and Hyperbolic Tangent](../Images/Logistic_Sigmoid_and_Hyperbolic_Tangent.png)
   >
   > - As shown above, the hyperbolic tangent function is a rescaling and stretching of the logistic sigmoid function with an output range of $(-1, 1)$, which can be thought of as a rescaled version of the latter. More precisely, the hyperbolic tangent function asymptotes at $\displaystyle{\lim_{x \to \infty} \tanh(x) = 1}$ and $\displaystyle{\lim_{x \to -\infty} \tanh(x) = -1}$, and crosses zero at $\tanh(x) = 0$. The formula is deduced as follows.
   >
   > $$\tanh(x) = \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}} = \frac{e^{2x} - 1}{e^{2x} + 1} = 2 \cdot \frac{e^{2x}}{1 + e^{2x}} - 1 = 2 \cdot \sigma(2x) − 1$$
   >
   > The output of the hyperbolic tangent function is centered at the origin, which maps strongly positive (negative) inputs to positive (negative) outputs, respectively, with a codomain of $(-1, 1)$, and maps near-zero inputs to near-zero outputs, making the outputs near-zero in mean, which helps in centering the data. This allows for better weight initialization and faster convergence during training, which is an advantage over logistic sigmoid functions.
   >
   > - The derivative of the hyperbolic tangent function is also symmetric with respect to the y-axis, which has a maximum at $\frac{d}{dx} \tanh(0) = 1$, with a codomain of $(0, 1]$. The derivative formula is deduced as follows.
   >
   > $$\frac{d}{dx} \tanh(x) = \frac{(e^{x} + e^{-x}) \cdot \frac{d}{dx} (e^{x} - e^{-x}) - (e^{x} - e^{-x}) \cdot \frac{d}{dx} (e^{x} + e^{-x})}{(e^{x} + e^{-x})^2} = \frac{(e^{x} + e^{-x})^2 - (e^{x} - e^{-x})^2}{(e^{x} + e^{-x})^2} = 1 - \left( \frac{e^{x} - e^{-x}}{e^{x} + e^{-x}} \right) ^2 = 1 - \tanh^2(x)$$
   >
   > - The gradient of the hyperbolic tangent function is steeper than that of the logistic sigmoid function, especially near the origin. The steeper gradient enables the network to obtain higher gradient values during backpropagation, thereby mitigating the vanishing gradient problem to some extent (which may still occur in deep networks), and fastening the learning and convergence of the network. This is another advantage over the latter.
   >
   > - However, activation functions with saturated regions, such as the logistic sigmoid function and the hyperbolic tangent function, always fail to completely avoid the vanishing gradient problem during backpropagation.

- The `torch.nn.BCELoss` class: creates a criterion that measures the binary cross-entropy between the target and the input probabilities.
  
  - The concept of information entropy was introduced by Claude Shannon in his monumental 1948 paper "A Mathematical Theory of Communication," hence it is also known as Shannon entropy. In information theory, the information entropy (or simply entropy) is a mathematical function that intuitively gives a measure of the uncertainty of the random variable; the larger the entropy, the less a priori information one has on the value of the random variable. In information theoretic terms, the information entropy of a system is the amount of "missing" information needed to determine a microstate, given the macrostate.

  - In information theory, the information content is a basic quantity derived from the probability of a particular event occurring from a random variable, also known as Shannon information content. In other words, for an ensemble $X$ that can be represented as a triple $(x, \mathcal{A}_{X}, \mathcal{P}_{X})$, the information content $h(a_{i})$ is a natural measure of the information content of the event $x = a_{i}$, and hence the name of this quantity can be shortened to the self-information. The information content of an outcome $x$ is defined as follows.

    $$h(x) = \log_{b}\frac{1}{P(x)} = -\log_{b}P(x),$$

    where the outcome $x$ is the value of a random variable, which takes on one of a set of possible values, $\mathcal{A}_{X} = \{a_{1}, a_{2}, \cdots , a_{i}, \cdots , a_{I}\}$, having probabilities $\mathcal{P}_{X} = \{p_{1}, p_{2}, \cdots , p_{i}, \cdots , p_{I}\}$, with $P(x = a_{i}) = p_{i} \geq 0$ and $\displaystyle{\sum_{a_{i} \in \mathcal{A}_{X}}} P(x = a_{i}) = 1$. Note that, the name $\mathcal{A}$ is mnemonic for "alphabet," and $P(x = a_{i})$ may be written as $P(a_{i})$ or $P(x)$ for abbreviations. In addition, $b$ denotes the base of the logarithm, unless it is used to simplify the formula if necessary, which is currently not critical in this series of definitions, since it only affects the value by a multiplicative constant, and the common value of $b$ is $2$, which means measured in bits.

  - The information content is closely related to the entropy, which is the average amount of self-information that an observer would expect to obtain when measuring a random variable, while the latter is the expected value of the self-information of a random variable, quantifying the degree of surprise that the random variable "on average" is, and hence one could also say that the entropy is the measure of surprise of a specific event/random variable. The information entropy of an ensemble $X$ can be defined as the average information content of the following outcome, which can also be referred to as the uncertainty of $X$.

    $$H(X) \equiv \displaystyle{\sum_{x \in \mathcal{A}_{X}}} P(x) \cdot \log_{b}\frac{1}{P(x)} = -\displaystyle{\sum_{x \in \mathcal{A}_{X}}} P(x) \cdot \log_{b}P(x) = \mathbb{E} [-\log_{b}P(x)] = \mathbb{E} [h(x)],$$

    with the convention for $P(x) = 0$ that $0 \times \log \frac{1}{0} \equiv 0$, since $\lim_{\theta \rightarrow 0^+} \theta \log \frac{1}{\theta} = 0$.

  - The cross-entropy is a fundamental concept in information theory builds upon the idea of the information entropy and calculates the number of bits required to represent or transmit an average event from one distribution compared to another distribution. In simple terms, if there exists a target or underlying probability distribution (the true probability distribution) $P$ and an approximation of the target distribution (the predicted probability distribution) $Q$, then the cross-entropy of $Q$ from $P$ is the number of additional bits to represent an event using $Q$ instead of $P$. The cross-entropy between two probability distributions, such as Q from P, can be defined formally defined as follows.

    $$H(P, Q) = \displaystyle{\sum_{x \in \mathcal{A}_{X}}} P(x) \cdot \log_{b}\frac{1}{Q(x)} = -\displaystyle{\sum_{x \in \mathcal{A}_{X}}} P(x) \cdot \log_{b}Q(x),$$

    where $P(x)$ is the probability of event $x = a_{i}$ in the true probability distribution $\mathcal{P}_{X}$, and $Q(x)$ is the probability of event $x = a_{i}$ in the predicted probability distribution $\mathcal{Q}_{X}$.

  - The cross-entropy can be used to define a loss function in machine learning and optimization that is called the cross-entropy (CE) loss, also known as the log loss (or logarithmic loss or logistic loss) which can be applied to both binary and multi-class classification problems. The cross-entropy loss measures the performance of a classification model whose output is a probability value between $0$ and $1$ and which increases as the predicted probability diverges from the actual label, as defined below (as the average loss over all samples).

    $$L_{CE} = -\frac{1}{N} \sum_{i = 1}^{N} \sum_{j = 1}^{C} y_{ij} \cdot \log_{b}\hat{y}_{ij},$$

    where $N$ is the number of samples, $C$ is the number of classes, $y_{ij}$ is the true output for the $i$-th sample and $j$-th class, and $\hat{y}_{ij}$ is the predicted output of the current model for the $i$-th sample and $j$-th class.

  - More specifically, the binary cross-entropy (BCE) loss function, can be used to classify two possible classes from a binary regression model. And unlike in the multinomial logistic regression, in the binary (or binomial) logistic regression, the predicted probability distribution is not modeled using the softmax function, but is usually modeled using the logistic sigmoid function. Therefore, the definition of the binary cross-entropy loss function can be rewritten by the above cross-entropy loss function as follows.

    $$L_{BCE} = -\frac{1}{N} \sum_{i = 1}^{N} [y_{i} \cdot \log_{b}\hat{y}_{i} + (1 - y_{i}) \cdot \log_{b}(1 - \hat{y}_{i})],$$

    where $N$ is still the number of samples, $y_{i}$ is the true output for the $i$-th sample, and $\hat{y}_{i}$ is the predicted output of the current model for the $i$-th sample, which comes from a probability distribution modeled as the logistic sigmoid function.

- Using this recombined class at the input layer is numerically more stable than using a plain sigmoid function (the `torch.nn.Sigmoid` class) followed by the loss function based on the binary cross-entropy criterion (the `torch.nn.BCELoss` class), since combining these operations into a single layer facilitates the use of the log-sum-exp trick to improve numerical stability.

  - When implementing the binary cross-entropy loss, if the logistic sigmoid function $\sigma(x_{i})$ is computed first, and then inserting $\hat{y}_{i} = \sigma(x_{i})$ into the definition of the binary cross-entropy loss, at this point it will be found in practice that there is a problem: at the beginning of the training, a positive example may be confidently classified as a negative example at the beginning of training ($x_{i} \ll 0$, which means $y_{i} \approx 0$). However, if $y_{i}$ is small enough, it may be less than the smallest floating-point value, i.e., have a value of $0$. Then, if taking the logarithm of $0$ in the calculation of the binary cross-entropy loss, it gives $-\infty$, i.e., arithmetic underflow.

  - To tackle this potential numerical stability issue, the TensorFlow and PyTorch libraries usually combine the logistic sigmoid function and the binary cross-entropy loss into one. As shown below, the logistic sigmoid function is introduced into the binary cross-entropy loss formula and simplified by the quotient rule for logarithms $\log \frac{m}{n} = \log m - \log n$ and the power rule for logarithms $\log m^{k} = k \log m$ to obtain the simplified formula. However, the numerical stability problem is not completely controlled since if $x_{i}$ is a large negative number, $e^{-x_{i}}$ blow up, i.e., arithmetic overflow.
  
    \begin{align}
    L_{BCE} &= -\frac{1}{N} \sum_{i = 1}^{N} \left[y_{i} \cdot \log_{b} \frac{1}{1 + e^{-x_{i}}} + (1 - y_{i}) \cdot \log_{b} (1 - \frac{1}{1 + e^{-x_{i}}})\right] \notag \\
    &= -\frac{1}{N} \sum_{i = 1}^{N} [-y_{i} \cdot \log_{b} (1 + e^{-x_{i}}) + (1 - y_{i}) \cdot (\log_{b} e^{-x_{i}} - \log_{b} (1 + e^{-x_{i}}))] \notag \\
    &= -\frac{1}{N} \sum_{i = 1}^{N} [-y_{i} \cdot \log_{b} (1 + e^{-x_{i}}) - y_{i} \cdot \log_{b} e^{-x_{i}} + y_{i} \cdot \log_{b} (1 + e^{-x_{i}}) + \log_{b} e^{-x_{i}} - \log_{b} (1 + e^{-x_{i}})] \notag \\
    &= -\frac{1}{N} \sum_{i = 1}^{N} [-y_{i} \cdot \log_{b} e^{-x_{i}} + \log_{b} e^{-x_{i}} - \log_{b} (1 + e^{-x_{i}})] \notag \\
    &= -\frac{1}{N} \sum_{i = 1}^{N} [x_{i} \cdot y_{i} \log_{b} e - x_{i} \cdot \log_{b} e - \log_{b} (1 + e^{-x_{i}})] \notag
    \end{align}

  - To solve this potential problem, the log-sum-exp (LSE) trick is used to shift the center of the exponential sum, which is described as follows. Taking the following equation as an example, using the log-sum-exp trick forces the greatest value to be zero, even if other values would underflow. A typical manner of setting the value is to set an arbitrary $\alpha$ to its maximum $\alpha = \max\limits_{i} x_{i}$ so as to get a reasonable result.
  
   $$\log_{e} \sum_{i = 1}^{N} e^{x_{i}} = \alpha + \log_{e} \sum_{i = 1}^{N} e^{x_{i}} - \alpha = \alpha + \log_{e} \sum_{i = 1}^{N} e^{x_{i}} - \log_{e} e^{\alpha} = \alpha + \log_{e} \sum_{i = 1}^{N} \frac{e^{x_{i}}}{e^{\alpha}} = \alpha + \log_{e} \sum_{i = 1}^{N} e^{x_{i} - \alpha}$$
  
  - It should be noted that before using the log-sum-exp trick, the base of the default logarithm should be set to Euler's number $e$, so that it is convenient to convert $\log_{b} b = 1$ when the argument and base of the logarithm are the same, and also to unify with the base of the logarithm in the log-sum-exp trick, which makes it easy to integrate them better.
  
   $$L_{BCE} = -\frac{1}{N} \sum_{i = 1}^{N} [x_{i} \cdot y_{i} \log_{e} e - x_{i} \cdot \log_{e} e - \log_{e} (1 + e^{-x_{i}})] = \frac{1}{N} \sum_{i = 1}^{N} [\log_{e} (1 + e^{-x_{i}}) + x_{i} (1 - y_{i})]$$
  
  - After unifying the base of the logarithm, the formula can be further simplified using the log-sum-exp trick, as shown below. In addition to this, a very interesting discovery was noted that PyTorch uses the negative of the rectified linear unit (-ReLU) as $\alpha$, i.e., $\alpha = \max (-x_{i}, 0) = \begin{cases}0 &if\ x_{i} \geq 0,\\-x_{i} &otherwise.\end{cases}$, which is basically a clever way of avoiding positive exponents and thus avoiding overflow.

    \begin{align}
    L_{BCE} &= \frac{1}{N} \sum_{i = 1}^{N} [\log_{e} (1 + e^{-x_{i}}) + x_{i} (1 - y_{i}) + \log_{e} e^{-\alpha} + \alpha] \notag \\
    &= \frac{1}{N} \sum_{i = 1}^{N} [\log_{e} (1 + e^{-x_{i}}) \cdot e^{-\alpha} + x_{i} (1 - y_{i}) + \alpha] \notag \\
    &= \frac{1}{N} \sum_{i = 1}^{N} [\log_{e} (e^{-\alpha} + e^{-x_{i} - \alpha}) + x_{i} (1 - y_{i}) + \alpha] \notag
    \end{align}

> **Tips**

> The log-sum-exp trick helps prevent underflow/overflow errors, and is in essence just taking advantage of mathematical properties to reduce underflow/overflow by using the log-sum-exp function which computes a smoothed maximum, i.e., a smoothed approximation of the maximum value function, and is mainly used in machine learning algorithms.

***

# Based on empirical studies and experience in training a model on these processed datasets, it has been found that using pre-training weights leads to early saturation of the model's learning capacity, at which point the model stops learning about the general problem and begins to learn about the data, thus showing a decrease in the training loss and an increase in the validation loss, i.e., overfitting

# Since the images in these datasets are not default RGB color images, but grayscale images, the model's first convolutional layer needs to be changed from the original 3 input channels to 1 input channel

# Whether in the `torch.nn.Conv2d` class or the `torch.nn.Linear` class, the argument `bias` defaults to True, indicating whether the layer learns an additive bias

# Since both processed datasets contain only one binary class label, the argument `out_features` for the last layer of the model, i.e., the fully connected (FC) layer, needs to be changed from 1000 to 1, which refers to the Boolean result of the binary class label
